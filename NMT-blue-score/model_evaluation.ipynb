{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eunwo\\OneDrive\\바탕 화면\\Dev\\KoBART-translation\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model. c:\\Users\\eunwo\\OneDrive\\바탕 화면\\Dev\\KoBART-translation\\.cache\\kobart_base_tokenizer_cased_cf74400bce.zip\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가하기\n",
    "# 1. 모델 불러오기\n",
    "import torch\n",
    "from kobart import get_kobart_tokenizer\n",
    "from transformers.models.bart import BartForConditionalGeneration\n",
    "\n",
    "def load_model():\n",
    "    model = BartForConditionalGeneration.from_pretrained('./kobart_translation')\n",
    "    # tokenizer = get_kobart_tokenizer()\n",
    "    return model\n",
    "\n",
    "model = load_model()\n",
    "tokenizer = get_kobart_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 모델 함수 정의하기\n",
    "def translate(text:str) -> str:\n",
    "    input_ids = tokenizer.encode(text)\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    input_ids = input_ids.unsqueeze(0)\n",
    "    output = model.generate(input_ids, eos_token_id=1, max_length=512, num_beams=5)\n",
    "    output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 1200000/1200000 [00:00<00:00, 1200068.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data has been saved to ko_converted_data_cleaned.json\n"
     ]
    }
   ],
   "source": [
    "# 중복 데이터 제거하기.\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "input_file = 'ko_converted_data.json'\n",
    "output_file = 'ko_converted_data_cleaned.json'\n",
    "\n",
    "# ko_converted_data.json 파일 불러오기\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 중복된 데이터 제거하기\n",
    "unique_data = []\n",
    "seen_sentences = set()\n",
    "\n",
    "with tqdm(total=len(data), desc='Processing') as pbar:\n",
    "    for item in data:\n",
    "        ko_original = item['ko_original']\n",
    "        ko_converted = item['ko_converted']\n",
    "\n",
    "        if ko_original != ko_converted and ko_original not in seen_sentences:\n",
    "            unique_data.append(item)\n",
    "            seen_sentences.add(ko_original)\n",
    "        pbar.update(1)\n",
    "\n",
    "# 중복 제거된 데이터 저장하기\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(unique_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Cleaned data has been saved to\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eunwo\\OneDrive\\바탕 화면\\Dev\\KoBART-translation\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "  0%|          | 73/111813 [00:01<50:30, 36.88it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m     en_sentence \u001b[39m=\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39men\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     50\u001b[0m     \u001b[39mif\u001b[39;00m ko_sentence \u001b[39min\u001b[39;00m ko_origin_sentences:\n\u001b[1;32m---> 51\u001b[0m         matched_sentences\u001b[39m.\u001b[39;49mappend({\u001b[39m'\u001b[39m\u001b[39mkr\u001b[39m\u001b[39m'\u001b[39m: ko_sentence, \u001b[39m'\u001b[39m\u001b[39men\u001b[39m\u001b[39m'\u001b[39m: en_sentence})\n\u001b[0;32m     54\u001b[0m \u001b[39m# 동일한 문장들을 TSV 파일로 저장한다.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(output_file, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m, newline\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "import json\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm.contrib.concurrent import thread_map\n",
    "\n",
    "#3-1. 테스트할 데이터 분리하기. (원본 문장과 노이즈가 적용된 문장 분리)\n",
    "#I: 원본만 분리 (100 개)\n",
    "tsv_file = 'data/validation.tsv'\n",
    "json_file = './ko_converted_data_cleaned.json'\n",
    "output_file = './data(test)/train_origin.tsv'\n",
    "num_samples = 100  # 저장할 랜덤 샘플 개수\n",
    "\n",
    "\n",
    "# 'ko_converted.json' 파일에서 필요한 값을 불러온다.\n",
    "with open(json_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "ko_origin_sentences = [item['ko_original'] for item in data]\n",
    "\n",
    "matched_sentences = []\n",
    "\n",
    "# 'validation.tsv' 파일을 읽어와서 'ko' 항목을 검사한다.\n",
    "with open(tsv_file, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f, delimiter='\\t')\n",
    "    sentences = [row for row in reader]  # 모든 문장을 리스트로 저장\n",
    "\n",
    "# 랜덤하게 100개의 문장을 선택하여 'ko' 항목과 'ko_converted' 값을 비교하고, 동일한 문장인 경우 저장한다.\n",
    "# random.shuffle(sentences)\n",
    "# count = 0\n",
    "# with tqdm(total=num_samples, desc='Processing') as pbar:\n",
    "#     for row in sentences:\n",
    "#         ko_sentence = row['kr']\n",
    "#         en_sentence = row['en']\n",
    "\n",
    "#         if ko_sentence in ko_origin_sentences:\n",
    "#             matched_sentences.append({'kr': ko_sentence, 'en': en_sentence})\n",
    "#             count += 1\n",
    "#             pbar.update(1)\n",
    "\n",
    "#         if count == num_samples:\n",
    "#             break\n",
    "\n",
    "#이건 특수 상황\n",
    "for row in tqdm(sentences):\n",
    "    ko_sentence = row['kr']\n",
    "    en_sentence = row['en']\n",
    "\n",
    "    if ko_sentence in ko_origin_sentences:\n",
    "        matched_sentences.append({'kr': ko_sentence, 'en': en_sentence})\n",
    "\n",
    "\n",
    "# 동일한 문장들을 TSV 파일로 저장한다.\n",
    "with open(output_file, 'w', encoding='utf-8', newline='') as f:\n",
    "    fieldnames = ['kr', 'en']\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, delimiter='\\t')\n",
    "    writer.writeheader()\n",
    "    writer.writerows(matched_sentences)\n",
    "\n",
    "print(\"Matched sentences(type I) have been saved to\", output_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 100/100 [00:07<00:00, 13.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched sentences(type II) have been saved to ./data(test)/data_type_II.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "import json\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from tqdm import tqdm\n",
    "\n",
    "#3-1. 테스트할 데이터 분리하기. (원본 문장과 노이즈가 적용된 문장 분리)\n",
    "#II: 노이즈만 분리 (100 개)\n",
    "tsv_file = 'data/validation.tsv'\n",
    "json_file = './ko_converted_data_cleaned.json'\n",
    "output_file = './data(test)/data_type_II.tsv'\n",
    "num_samples = 100  # 저장할 랜덤 샘플 개수\n",
    "\n",
    "\n",
    "# 'ko_converted.json' 파일에서 필요한 값을 불러온다.\n",
    "with open(json_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "ko_converted_sentences = [item['ko_converted'] for item in data]\n",
    "\n",
    "matched_sentences = []\n",
    "\n",
    "# 'validation.tsv' 파일을 읽어와서 'ko' 항목을 검사한다.\n",
    "with open(tsv_file, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f, delimiter='\\t')\n",
    "    sentences = [row for row in reader]  # 모든 문장을 리스트로 저장\n",
    "\n",
    "# 랜덤하게 100개의 문장을 선택하여 'ko' 항목과 'ko_converted' 값을 비교하고, 동일한 문장인 경우 저장한다.\n",
    "random.shuffle(sentences)\n",
    "count = 0\n",
    "with tqdm(total=num_samples, desc='Processing') as pbar:\n",
    "    for row in sentences:\n",
    "        ko_sentence = row['kr']\n",
    "        en_sentence = row['en']\n",
    "\n",
    "        if ko_sentence in ko_converted_sentences:\n",
    "            matched_sentences.append({'kr': ko_sentence, 'en': en_sentence})\n",
    "            count += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "        if count == num_samples:\n",
    "            break\n",
    "\n",
    "# 동일한 문장들을 TSV 파일로 저장한다.\n",
    "with open(output_file, 'w', encoding='utf-8', newline='') as f:\n",
    "    fieldnames = ['kr', 'en']\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, delimiter='\\t')\n",
    "    writer.writeheader()\n",
    "    writer.writerows(matched_sentences)\n",
    "\n",
    "print(\"Matched sentences(type II) have been saved to\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 100/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched sentences(type III) have been saved to ./data(test)/data_type_III.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#III: 기존에서 I, II의 데이터를 제외해서 (100 개)\n",
    "import json\n",
    "import csv\n",
    "import random\n",
    "\n",
    "validation_file = './data/validation.tsv'\n",
    "data_type_I_file = './data(test)/data_type_I.tsv'\n",
    "data_type_II_file = './data(test)/data_type_II.tsv'\n",
    "output_file = './data(test)/data_type_III.tsv'\n",
    "num_samples = 100  # 저장할 랜덤 샘플 개수\n",
    "\n",
    "# validation.tsv 파일에서 랜덤하게 num_samples 개의 샘플 추출\n",
    "with open(validation_file, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f, delimiter='\\t')\n",
    "    validation_sentences = [row for row in reader]\n",
    "\n",
    "random.shuffle(validation_sentences)\n",
    "\n",
    "# data_type_I.tsv 파일에서 중복된 문장 제거\n",
    "with open(data_type_I_file, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f, delimiter='\\t')\n",
    "    data_type_I_sentences = {row['kr'] for row in reader}\n",
    "\n",
    "# data_type_II.tsv 파일에서 중복된 문장 제거\n",
    "with open(data_type_II_file, 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f, delimiter='\\t')\n",
    "    data_type_II_sentences = {row['kr'] for row in reader}\n",
    "\n",
    "matched_sentences = []\n",
    "\n",
    "count = 0\n",
    "with tqdm(total=num_samples, desc='Processing') as pbar:\n",
    "    for row in validation_sentences:\n",
    "        ko_sentence = row['kr']\n",
    "        en_sentence = row['en']\n",
    "\n",
    "        if ko_sentence not in data_type_I_sentences and ko_sentence not in data_type_II_sentences:\n",
    "            matched_sentences.append({'kr': ko_sentence, 'en': en_sentence})\n",
    "            count += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "        if count == num_samples:\n",
    "            break\n",
    "\n",
    "# 동일한 문장들을 TSV 파일로 저장한다.\n",
    "with open(output_file, 'w', encoding='utf-8', newline='') as f:\n",
    "    fieldnames = ['kr', 'en']\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames, delimiter='\\t')\n",
    "    writer.writeheader()\n",
    "    writer.writerows(matched_sentences)\n",
    "\n",
    "print(\"Matched sentences(type III) have been saved to\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_type_I 번역 시작\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:50<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_type_II 번역 시작\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:49<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_type_III 번역 시작\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:54<00:00,  1.15s/it]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_data(input_file):\n",
    "    # 입력 파일을 불러오기\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f, delimiter='\\t')\n",
    "        sentences = [row for row in reader]\n",
    "\n",
    "    model_outputs = {\"ref\": [], \"model\": []}\n",
    "\n",
    "    for row in tqdm(sentences):\n",
    "        model_outputs[\"model\"].append(translate(row['kr']))\n",
    "        model_outputs[\"ref\"].append(row['en'])\n",
    "        \n",
    "    return model_outputs, sentences\n",
    "\n",
    "# data_type_I.tsv 파일을 처리하여 모델 출력 구하기\n",
    "print('data_type_I 번역 시작')\n",
    "input_file_I = './data(test)/data_type_I.tsv'\n",
    "output_data_I, input_set_I = process_data(input_file_I)\n",
    "\n",
    "# data_type_II.tsv 파일을 처리하여 모델 출력 구하기\n",
    "print('data_type_II 번역 시작')\n",
    "input_file_II = './data(test)/data_type_II.tsv'\n",
    "output_data_II, input_set_II = process_data(input_file_II)\n",
    "\n",
    "print('data_type_III 번역 시작')\n",
    "input_file_III = './data(test)/data_type_III.tsv'\n",
    "output_data_III, input_set_III = process_data(input_file_III)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "#평가 함수 만들기\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_model_predictions(ref_data, model_output):\n",
    "    print(\"Evaluating model predictions.\")\n",
    "    total_bleu_score = 0.0\n",
    "    scores = []\n",
    "\n",
    "    for i, item in enumerate(ref_data):\n",
    "        en_text = item\n",
    "        translation = model_output[i]\n",
    "\n",
    "        # 두 항목간의 BLEU 평가를 진행한다. 경계 스무딩 사용해서 평가 진행\n",
    "        reference = [en_text.split()]\n",
    "        candidate = translation.split()\n",
    "        bleu_score = sentence_bleu(reference, candidate, smoothing_function=SmoothingFunction().method3)\n",
    "        scores.append(bleu_score)\n",
    "        total_bleu_score += bleu_score\n",
    "\n",
    "    average_bleu_score = total_bleu_score / len(ref_data)\n",
    "    print(\"Average BLEU score:\", average_bleu_score)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model predictions.\n",
      "Average BLEU score: 0.45602219142455347\n",
      "Evaluating model predictions.\n",
      "Average BLEU score: 0.4244658708624342\n",
      "Evaluating model predictions.\n",
      "Average BLEU score: 0.34034082576848945\n",
      "7, 1.0: Thank you for giving me this opportunity., Thank you for giving me this opportunity.\n",
      "11, 1.0: The biggest feature is that it can be used as a treatment and hair essence., The biggest feature is that it can be used as a treatment and hair essence.\n",
      "16, 1.0: We'll pick the final three first., We'll pick the final three first.\n",
      "20, 1.0: Chronic periodontal disease reduces respiratory function., Chronic periodontal disease reduces respiratory function.\n",
      "30, 1.0: For more information, please contact our call center., For more information, please contact our call center.\n",
      "12, 1.0: We knew that the sales negotiations are ongoing, so we are looking for a new supplier, but we haven't found it yet., > What is this?\n",
      "19, 1.0: Then you can use FFF2., I am sending you an e-mail to inform you that the business hours have changed.\n",
      "23, 1.0: The gel foam has a cooling property that reduces sleep by 1 degree or 2 degrees, absorbing heat from your pets., The wholesale price for fire extinguisher is 35 dollars each.\n",
      "32, 1.0: To give you an answer about pet accompanions, I'm sorry, you can't be accompanied., I like the packaging and designs on the label.\n",
      "37, 1.0: I really couldn't have solved it without your help., The application works with the help of remote control.\n",
      "15, 1.0: Like all smart homes, my house has smart TVs, washing machines, refrigerators, and many more., You can easily capture without using your hands.\n",
      "46, 1.0: >I prepared everything to eat., >I won't do this.\n",
      "60, 1.0: I'll contact you again when I send you my opinion on the review., Online learning tends to focus on efficiency.\n",
      "73, 1.0: BBB1. We were established in 2011 as a separate corporation from BBB2., Attached is a copy of our order statement.\n"
     ]
    }
   ],
   "source": [
    "#평가 진행\n",
    "score_I = evaluate_model_predictions(output_data_I['ref'], output_data_I['model'])\n",
    "score_II = evaluate_model_predictions(output_data_II['ref'], output_data_II['model'])\n",
    "score_III = evaluate_model_predictions(output_data_III['ref'], output_data_III['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model predictions.\n",
      "Average BLEU score: 0.3646768046296858\n",
      "Evaluating model predictions.\n",
      "Average BLEU score: 0.13391106056343174\n",
      "Evaluating model predictions.\n",
      "Average BLEU score: 0.24450656731965015\n",
      "Evaluating model predictions.\n",
      "Average BLEU score: 0.44699019034733084\n",
      "Evaluating model predictions.\n",
      "Average BLEU score: 0.12864758485808458\n",
      "Evaluating model predictions.\n",
      "Average BLEU score: 0.28168597388759037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2636932645412712,\n",
       " 0.3079300751569292,\n",
       " 0.3827521065936582,\n",
       " 0.43167001068522526,\n",
       " 0.05795599612995367,\n",
       " 0.031251907639724415,\n",
       " 0.7292571723872933,\n",
       " 0.16449759298465816,\n",
       " 0.8091067115702212,\n",
       " 0.5214865845309236,\n",
       " 0.03716499092256817,\n",
       " 0.6606328636027614,\n",
       " 0.08116697886877472,\n",
       " 0.025612540390806925,\n",
       " 0.10937121222607606,\n",
       " 0.5444460596606694,\n",
       " 0.3128974322923816,\n",
       " 0.6606328636027614,\n",
       " 0.4989070972910272,\n",
       " 0.1561969968460128,\n",
       " 0.13065113298388567,\n",
       " 0,\n",
       " 0.04814971807094068,\n",
       " 0.5169731539571706,\n",
       " 0.28319415510892393,\n",
       " 0.15415064977510756,\n",
       " 0,\n",
       " 0,\n",
       " 0.06496183867338828,\n",
       " 0.537284965911771,\n",
       " 1.0,\n",
       " 0.20235553926673694,\n",
       " 0.43724109850912707,\n",
       " 0.041961149062965476,\n",
       " 0.4518010018049224,\n",
       " 0.04932351569489709,\n",
       " 0.08310415003234632,\n",
       " 0.14323145079400493,\n",
       " 0.2259005009024612,\n",
       " 0.2767783451247154,\n",
       " 0.2259005009024612,\n",
       " 0.06722636787666482,\n",
       " 0,\n",
       " 0.5578002860768766,\n",
       " 0.2557539057896621,\n",
       " 0,\n",
       " 0.3535533905932738,\n",
       " 0.22637359354764466,\n",
       " 0.31702331385234306,\n",
       " 0.7910665071754358,\n",
       " 0.21105340631872635,\n",
       " 0.6340466277046861,\n",
       " 0,\n",
       " 0.09917720727091445,\n",
       " 0.46761439722697934,\n",
       " 0.025540496664715903,\n",
       " 0.15727800941615355,\n",
       " 0.40443388950530595,\n",
       " 0.08116697886877472,\n",
       " 0.3508439695638686,\n",
       " 0,\n",
       " 0.09980099403873663,\n",
       " 0.05328642242777789,\n",
       " 0.05868924818816534,\n",
       " 0.34172334076593075,\n",
       " 0.16784459625186196,\n",
       " 0,\n",
       " 0.06916271812933181,\n",
       " 0.8371170098777919,\n",
       " 0,\n",
       " 0.22869470664051195,\n",
       " 0.48633831680799433,\n",
       " 0.2623762003935851,\n",
       " 0.5873949094699213,\n",
       " 0.4324227075463215,\n",
       " 1.0,\n",
       " 0.3073940764756322,\n",
       " 0.21191828141393898,\n",
       " 0.16767849550785174,\n",
       " 1.0,\n",
       " 0.24640511033537824,\n",
       " 0.13134549472120793,\n",
       " 0.1392442062500076,\n",
       " 0.2111187176080899,\n",
       " 0.19304869754804482,\n",
       " 0.02273154356702287,\n",
       " 0.2231618068926664,\n",
       " 0,\n",
       " 0.8408964152537145,\n",
       " 0.32372956394183194,\n",
       " 0.2044800736021839,\n",
       " 0.4989070972910272,\n",
       " 0,\n",
       " 0.09238430210261096,\n",
       " 0.04266331692956902,\n",
       " 0.8408964152537145,\n",
       " 0.038685645292085825,\n",
       " 0.30661487102926754,\n",
       " 0,\n",
       " 0.8408964152537145]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# JSON 파일 불러오기\n",
    "with open(\"./data_google_papago/responses_google_data_i.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
    "    g_data_1 = json.load(json_file)\n",
    "with open(\"./data_google_papago/responses_google_data_ii.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
    "    g_data_2 = json.load(json_file)\n",
    "with open(\"./data_google_papago/responses_google_data_iii.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
    "    g_data_3 = json.load(json_file)\n",
    "with open(\"./data_google_papago/responses_papago_data_i.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
    "    n_data_1 = json.load(json_file)\n",
    "with open(\"./data_google_papago/responses_papago_data_ii.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
    "    n_data_2 = json.load(json_file)\n",
    "with open(\"./data_google_papago/responses_papago_data_iii.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
    "    n_data_3 = json.load(json_file)\n",
    "\n",
    "\n",
    "# key와 value 분리\n",
    "g_values1 = list(g_data_1.values())\n",
    "g_values2 = list(g_data_2.values())\n",
    "g_values3 = list(g_data_3.values())\n",
    "n_values1 = list(n_data_1.values())\n",
    "n_values2 = list(n_data_2.values())\n",
    "n_values3 = list(n_data_3.values())\n",
    "\n",
    "evaluate_model_predictions(output_data_I['ref'], g_values1)\n",
    "evaluate_model_predictions(output_data_II['ref'], g_values2)\n",
    "evaluate_model_predictions(output_data_III['ref'], g_values3)\n",
    "evaluate_model_predictions(output_data_I['ref'], n_values1)\n",
    "evaluate_model_predictions(output_data_II['ref'], n_values2)\n",
    "evaluate_model_predictions(output_data_III['ref'], n_values3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model predictions.\n",
      "Average BLEU score: 0.45602219142455347\n",
      "Evaluating model predictions.\n",
      "Average BLEU score: 0.4244658708624342\n",
      "Evaluating model predictions.\n",
      "Average BLEU score: 0.34034082576848945\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'input_set_I' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m i, d \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(output_data_I[\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[0;32m      9\u001b[0m     \u001b[39mif\u001b[39;00m score_I[i] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m:\n\u001b[1;32m---> 10\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mscore_I[i]\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00md\u001b[39m}\u001b[39;00m\u001b[39m/ \u001b[39m\u001b[39m{\u001b[39;00moutput_data_I[\u001b[39m\"\u001b[39m\u001b[39mref\u001b[39m\u001b[39m\"\u001b[39m][i]\u001b[39m}\u001b[39;00m\u001b[39m / \u001b[39m\u001b[39m{\u001b[39;00minput_set_I[i]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mGoogle: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(g_data_1\u001b[39m.\u001b[39mvalues())[i]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mPapago: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(n_data_1\u001b[39m.\u001b[39mvalues())[i]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_set_I' is not defined"
     ]
    }
   ],
   "source": [
    "#평가 진행\n",
    "score_I = evaluate_model_predictions(output_data_I['ref'], output_data_I['model'])\n",
    "score_II = evaluate_model_predictions(output_data_II['ref'], output_data_II['model'])\n",
    "score_III = evaluate_model_predictions(output_data_III['ref'], output_data_III['model'])\n",
    "\n",
    "maximum = 3\n",
    "count = 0\n",
    "for i, d in enumerate(output_data_I['model']):\n",
    "    if score_I[i] <= 0.5:\n",
    "        print(f'{i}, {score_I[i]}: {d}/ {output_data_I[\"ref\"][i]} / {input_set_I[i]}')\n",
    "        print(f'Google: {list(g_data_1.values())[i]}')\n",
    "        print(f'Papago: {list(n_data_1.values())[i]}')\n",
    "        count += 1\n",
    "    if not (count < maximum):\n",
    "        break\n",
    "\n",
    "count = 0\n",
    "for i, d in enumerate(output_data_I['model']):\n",
    "    if score_II[i] >= 0.9999:\n",
    "        print(f'{i}, {score_II[i]}: {d}/ {output_data_II[\"ref\"][i]} / {input_set_II[i]}')\n",
    "        print(f'Google: {list(g_data_2.values())[i]}')\n",
    "        print(f'Papago: {list(n_data_2.values())[i]}')\n",
    "        count += 1\n",
    "    if not (count < maximum):\n",
    "        break\n",
    "\n",
    "count = 0\n",
    "for i, d in enumerate(output_data_I['model']):\n",
    "    if score_III[i] >= 0.9999:\n",
    "        print(f'{i}, {score_III[i]}: {d}/ {output_data_III[\"ref\"][i]} / {input_set_III[i]}')\n",
    "        print(f'Google: {list(g_data_3.values())[i]}')\n",
    "        print(f'Papago: {list(n_data_3.values())[i]}')\n",
    "        count += 1\n",
    "    if not (count < maximum):\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
